{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is written by Yara Al-Shorman\n",
    "# Date created: May 18 2022\n",
    "# Last modified: May 22 2022\n",
    "# Github repo: https://github.com/YaraAlShorman/Research-spring-22\n",
    "# The purpose of this code is to predict the bulkflows for a large given set of galaxies (using machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from numpy import load, concatenate, expand_dims  # I am doing this to minimize unncessary imports, they take up too much RAM and time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import floor\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking all of the data into one humungous tensor\n",
    "# I am using tensors because they can be ragged and can be stacked\n",
    "# these means we won't have to cut off our data to standardize array sizes\n",
    "data = load(f'C:/Users/yaras/Documents/Research/Feldman/rotated-outerrim-cz-rand/rotated-0-error-40.npy.npz')\n",
    "input_data = data['data']\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(input_data)\n",
    "input_data = scaler.transform(input_data)\n",
    "input_data = tf.expand_dims(input_data, axis=0)\n",
    "input_data = tf.RaggedTensor.from_tensor(input_data)\n",
    "# if scaling the data this way doesn't work good enough, i can try scaling all the data at once, to one scale\n",
    "# in this case, i am scaling every 'square' of data seprately\n",
    "# i should try scaling the entire 'cube' at once\n",
    "\n",
    "output_data = tf.convert_to_tensor(data['header'])\n",
    "output_data = tf.expand_dims(output_data, axis=0)\n",
    "\n",
    "num_of_files = 100 # variable for number of files to load, current limit is 3000\n",
    "\n",
    "for i in range(1, num_of_files): # limit 3000\n",
    "        array_data = load(f'C:/Users/yaras/Documents/Research/Feldman/rotated-outerrim-cz-rand/rotated-{i}-error-40.npy.npz')\n",
    "        # 'data' data\n",
    "        temp = tf.convert_to_tensor(array_data['data'], dtype='float64')\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(temp)\n",
    "        temp = scaler.transform(temp)\n",
    "        temp = tf.expand_dims(temp, axis=0)  # dimensions have to be expandad to be able to concat along the outer-dimension \n",
    "        input_data = tf.concat([input_data, temp], axis=0)  # concat adds to existing dimensions, does not create new ones\n",
    "        # 'input_data' is a tensor\n",
    "        \n",
    "        # 'header' data (bulkflows)\n",
    "        temp = array_data['header']\n",
    "        temp = expand_dims(temp, axis=0)\n",
    "        output_data = concatenate([output_data, temp], axis=0)\n",
    "        # 'output_data' is an array, because it is uniform\n",
    "        # i scale it (normalize it) all at once, then i covnert it to a tensor\n",
    "        \n",
    "# next is scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing/scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(output_data)\n",
    "output_data = scaler.transform(output_data)\n",
    "output_data = tf.convert_to_tensor(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train/test sections\n",
    "# 75% -> train\n",
    "# 25% -> test\n",
    "train_percent = 0.75\n",
    "input_train, input_test = (input_data[:floor(num_of_files * train_percent)], \\\n",
    "                                input_data[floor(num_of_files * train_percent)::])\n",
    "output_train, output_test = (output_data[:floor(num_of_files * train_percent)], \\\n",
    "                                output_data[floor(num_of_files * train_percent)::])\n",
    "\n",
    "# data is split and ready for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# # Place tensors on the CPU\n",
    "# with tf.device('/GPU:0'):\n",
    "#   a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "#   b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "# # Run on the GPU\n",
    "# c = tf.matmul(a, b)\n",
    "# print(c)\n",
    "\n",
    "# print(tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 31s 9s/step - loss: 0.5872\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 26s 8s/step - loss: 0.3331\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.2284\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 26s 8s/step - loss: 0.2064\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.1780\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 28s 9s/step - loss: 0.1664\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.1463\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.1313\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 29s 10s/step - loss: 0.1159\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.1050\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.0922\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.0850\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 26s 8s/step - loss: 0.0746\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 28s 9s/step - loss: 0.0734\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 39s 14s/step - loss: 0.0711\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 36s 10s/step - loss: 0.0660\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 29s 9s/step - loss: 0.0579\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 27s 9s/step - loss: 0.0607\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 28s 9s/step - loss: 0.0579\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 30s 10s/step - loss: 0.0581\n"
     ]
    }
   ],
   "source": [
    "# model creations\n",
    "\n",
    "model0 = tf.keras.Sequential()\n",
    "model0.add(tf.keras.layers.InputLayer(input_shape=[None, 4], ragged=True))\n",
    "model0.add(tf.keras.layers.LSTM(64, dropout=0.25, activation='selu'))\n",
    "model0.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model0.add(tf.keras.layers.Dropout(0.5))\n",
    "model0.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model0.add(tf.keras.layers.Dropout(0.5))\n",
    "model0.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model0.add(tf.keras.layers.Dropout(0.5))\n",
    "model0.add(tf.keras.layers.Dense(3, activation='linear'))\n",
    "\n",
    "model0.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "a = model0.fit(input_train, output_train, epochs=20, use_multiprocessing=True, workers=2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6038a1b420e1c960d5a71fceb90c2902e154518be2efe0b3c15ee4b8a34c8ba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
